{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/suvodeepmajumder/Conda/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,roc_auc_score,recall_score,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from pyearth import Earth\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from src import SMOTE\n",
    "from src import CFS\n",
    "from src import metrices_V2 as metrices\n",
    "\n",
    "import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(df):\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    cols = df.columns\n",
    "    smt = SMOTE.smote(df)\n",
    "    df = smt.run()\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def apply_cfs(df):\n",
    "        y = df.Bugs.values\n",
    "        X = df.drop(labels = ['Bugs'],axis = 1)\n",
    "        X = X.values\n",
    "        selected_cols = CFS.cfs(X,y)\n",
    "        cols = df.columns[[selected_cols]].tolist()\n",
    "        cols.append('Bugs')\n",
    "        return df[cols],cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_both_data(project,metric):\n",
    "    understand_path = 'data/understand_files_all/' + project + '_understand.csv'\n",
    "    understand_df = pd.read_csv(understand_path)\n",
    "    understand_df = understand_df.dropna(axis = 1,how='all')\n",
    "    cols_list = understand_df.columns.values.tolist()\n",
    "    for item in ['Kind', 'Name','commit_hash', 'Bugs']:\n",
    "        if item in cols_list:\n",
    "            cols_list.remove(item)\n",
    "            cols_list.insert(0,item)\n",
    "    understand_df = understand_df[cols_list]\n",
    "    cols = understand_df.columns.tolist()\n",
    "    understand_df = understand_df.drop_duplicates(cols[4:len(cols)])\n",
    "    understand_df['Name'] = understand_df.Name.str.rsplit('.',1).str[1]\n",
    "    \n",
    "    commit_guru_file_level_path = 'data/commit_guru_file/' + project + '.csv'\n",
    "    commit_guru_file_level_df = pd.read_csv(commit_guru_file_level_path)\n",
    "    commit_guru_file_level_df['commit_hash'] = commit_guru_file_level_df.commit_hash.str.strip('\"')\n",
    "    commit_guru_file_level_df = commit_guru_file_level_df[commit_guru_file_level_df['file_name'].str.contains('.java')]\n",
    "    commit_guru_file_level_df['Name'] = commit_guru_file_level_df.file_name.str.rsplit('/',1).str[1].str.split('.').str[0].str.replace('/','.')\n",
    "    commit_guru_file_level_df = commit_guru_file_level_df.drop('file_name',axis = 1)\n",
    "    \n",
    "    \n",
    "    df = understand_df.merge(commit_guru_file_level_df,how='left',on=['commit_hash','Name'])\n",
    "    \n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove('Bugs')\n",
    "    cols.append('Bugs')\n",
    "    df = df[cols]\n",
    "    \n",
    "    for item in ['Kind', 'Name','commit_hash']:\n",
    "        if item in cols:\n",
    "            df = df.drop(labels = [item],axis=1)\n",
    "#     df.dropna(inplace=True)\n",
    "    df = df.drop_duplicates()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    y = df.Bugs\n",
    "    X = df.drop('Bugs',axis = 1)\n",
    "    cols = X.columns\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    imp_mean = IterativeImputer(random_state=0)\n",
    "    X = imp_mean.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    \n",
    "    if metric == 'process':\n",
    "        X = X[['file_la', 'file_ld', 'file_lt', 'file_age', 'file_ddev',\n",
    "       'file_nuc', 'own', 'minor', 'file_ndev', 'file_ncomm', 'file_adev',\n",
    "       'file_nadev', 'file_avg_nddev', 'file_avg_nadev', 'file_avg_ncomm',\n",
    "       'file_ns', 'file_exp', 'file_sexp', 'file_rexp', 'file_nd', 'file_sctr']]\n",
    "    elif metric == 'product':\n",
    "        X = X.drop(['file_la', 'file_ld', 'file_lt', 'file_age', 'file_ddev',\n",
    "       'file_nuc', 'own', 'minor', 'file_ndev', 'file_ncomm', 'file_adev',\n",
    "       'file_nadev', 'file_avg_nddev', 'file_avg_nadev', 'file_avg_ncomm',\n",
    "       'file_ns', 'file_exp', 'file_sexp', 'file_rexp', 'file_nd', 'file_sctr'],axis = 1)\n",
    "    else:\n",
    "        X = X\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def load_data_commit_level(project,metric):\n",
    "    understand_path = 'data/understand_files_all/' + project + '_understand.csv'\n",
    "    understand_df = pd.read_csv(understand_path)\n",
    "    understand_df = understand_df.dropna(axis = 1,how='all')\n",
    "    cols_list = understand_df.columns.values.tolist()\n",
    "    for item in ['Kind', 'Name','commit_hash', 'Bugs']:\n",
    "        if item in cols_list:\n",
    "            cols_list.remove(item)\n",
    "            cols_list.insert(0,item)\n",
    "    understand_df = understand_df[cols_list]\n",
    "    cols = understand_df.columns.tolist()\n",
    "    understand_df = understand_df.drop_duplicates(cols[4:len(cols)])\n",
    "    understand_df['Name'] = understand_df.Name.str.rsplit('.',1).str[1]\n",
    "    \n",
    "    commit_guru_file_level_path = 'data/commit_guru_file/' + project + '.csv'\n",
    "    commit_guru_file_level_df = pd.read_csv(commit_guru_file_level_path)\n",
    "    commit_guru_file_level_df['commit_hash'] = commit_guru_file_level_df.commit_hash.str.strip('\"')\n",
    "    commit_guru_file_level_df = commit_guru_file_level_df[commit_guru_file_level_df['file_name'].str.contains('.java')]\n",
    "    commit_guru_file_level_df['Name'] = commit_guru_file_level_df.file_name.str.rsplit('/',1).str[1].str.split('.').str[0].str.replace('/','.')\n",
    "    commit_guru_file_level_df = commit_guru_file_level_df.drop('file_name',axis = 1)\n",
    "    \n",
    "    \n",
    "    df = understand_df.merge(commit_guru_file_level_df,how='left',on=['commit_hash','Name'])\n",
    "    \n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove('Bugs')\n",
    "    cols.append('Bugs')\n",
    "    df = df[cols]\n",
    "    commit_hash = df.commit_hash\n",
    "    for item in ['Kind', 'Name','commit_hash']:\n",
    "        if item in cols:\n",
    "            df = df.drop(labels = [item],axis=1)\n",
    "#     df.dropna(inplace=True)\n",
    "    df = df.drop_duplicates()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    y = df.Bugs\n",
    "    X = df.drop('Bugs',axis = 1)\n",
    "    cols = X.columns\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    imp_mean = IterativeImputer(random_state=0)\n",
    "    X = imp_mean.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    \n",
    "    if metric == 'process':\n",
    "        X = X[['file_la', 'file_ld', 'file_lt', 'file_age', 'file_ddev',\n",
    "       'file_nuc', 'own', 'minor', 'file_ndev', 'file_ncomm', 'file_adev',\n",
    "       'file_nadev', 'file_avg_nddev', 'file_avg_nadev', 'file_avg_ncomm',\n",
    "       'file_ns', 'file_exp', 'file_sexp', 'file_rexp', 'file_nd', 'file_sctr']]\n",
    "    elif metric == 'product':\n",
    "        X = X.drop(['file_la', 'file_ld', 'file_lt', 'file_age', 'file_ddev',\n",
    "       'file_nuc', 'own', 'minor', 'file_ndev', 'file_ncomm', 'file_adev',\n",
    "       'file_nadev', 'file_avg_nddev', 'file_avg_nadev', 'file_avg_ncomm',\n",
    "       'file_ns', 'file_exp', 'file_sexp', 'file_rexp', 'file_nd', 'file_sctr'],axis = 1)\n",
    "    else:\n",
    "        X = X\n",
    "    \n",
    "    df = X\n",
    "    df['Bugs'] = y\n",
    "    df['commit_hash'] = commit_hash\n",
    "    unique_commits = df.commit_hash.unique()\n",
    "    \n",
    "    train_df = df[df.commit_hash.isin(unique_commits[0:int(len(unique_commits)*.6)])]\n",
    "    test_df = df[df.commit_hash.isin(unique_commits[int(len(unique_commits)*.6):])]\n",
    "    \n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    test_df.reset_index(inplace=True, drop=True)\n",
    "    train_y = train_df.Bugs\n",
    "    train_X = train_df.drop(['Bugs','commit_hash'],axis = 1)\n",
    "    df.to_csv('/Users/suvodeepmajumder/Documents/AI4SE/TCKCCA/data/commit_guru_bubble/' + project + '.csv', index = False)\n",
    "    return train_X,train_y,test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035\n",
      "wrong Shape of passed values is (62, 45), indices imply (62, 66) spring-xd-samples\n",
      "wrong No columns to parse from file jgit\n",
      "wrong No columns to parse from file android-app\n",
      "wrong Shape of passed values is (77, 45), indices imply (77, 66) BottomSheetBuilder\n",
      "wrong Shape of passed values is (62, 45), indices imply (62, 66) community\n",
      "wrong Shape of passed values is (32, 45), indices imply (32, 66) gs-uploading-files\n",
      "wrong Shape of passed values is (244, 45), indices imply (244, 66) arara\n",
      "wrong Shape of passed values is (127, 45), indices imply (127, 66) SuperAdapter\n",
      "wrong [Errno 2] File data/commit_guru_file/android-material-design-icon-generator-plugin.csv does not exist: 'data/commit_guru_file/android-material-design-icon-generator-plugin.csv' android-material-design-icon-generator-plugin\n",
      "wrong Shape of passed values is (102, 45), indices imply (102, 66) JSONassert\n",
      "wrong Shape of passed values is (1037, 45), indices imply (1037, 66) jacoco\n",
      "wrong Shape of passed values is (5148, 45), indices imply (5148, 66) jersey\n",
      "wrong Shape of passed values is (39687, 45), indices imply (39687, 66) corretto-8\n",
      "wrong Shape of passed values is (2269, 45), indices imply (2269, 66) Pydev\n",
      "wrong Shape of passed values is (89, 45), indices imply (89, 66) app-icon\n",
      "wrong Shape of passed values is (40, 45), indices imply (40, 66) react-native-keychain\n",
      "wrong Shape of passed values is (160, 45), indices imply (160, 66) eo\n",
      "wrong Shape of passed values is (54, 45), indices imply (54, 66) Hacktoberfest-Mathematics\n",
      "wrong Shape of passed values is (1979, 45), indices imply (1979, 66) openiot\n",
      "wrong Shape of passed values is (86, 45), indices imply (86, 66) react-native-admob\n",
      "wrong Shape of passed values is (22915, 45), indices imply (22915, 66) groovy-eclipse\n",
      "wrong Shape of passed values is (201, 45), indices imply (201, 66) loklak_wok_android\n",
      "wrong Shape of passed values is (215, 45), indices imply (215, 66) addressbook-level2\n",
      "wrong Shape of passed values is (1008, 45), indices imply (1008, 66) limbo\n",
      "wrong Shape of passed values is (149, 45), indices imply (149, 66) react-native-baidu-map\n",
      "wrong Shape of passed values is (1253, 45), indices imply (1253, 66) quartz\n",
      "wrong Shape of passed values is (244, 45), indices imply (244, 66) PhotoPicker\n",
      "wrong Shape of passed values is (96, 45), indices imply (96, 66) react-native-screens\n",
      "wrong Shape of passed values is (204, 45), indices imply (204, 66) Lynx\n",
      "wrong Shape of passed values is (82653, 45), indices imply (82653, 66) netbeans\n",
      "wrong Shape of passed values is (102, 45), indices imply (102, 66) img2latex-mathpix\n",
      "wrong Shape of passed values is (70, 45), indices imply (70, 66) fragmentargs\n",
      "wrong Shape of passed values is (193, 45), indices imply (193, 66) tray\n",
      "wrong Shape of passed values is (257, 45), indices imply (257, 66) cm_api\n",
      "wrong Shape of passed values is (1598, 45), indices imply (1598, 66) cardslib\n",
      "wrong Shape of passed values is (500, 45), indices imply (500, 66) Emoji\n",
      "wrong Shape of passed values is (63, 45), indices imply (63, 66) algos\n",
      "wrong Shape of passed values is (455, 45), indices imply (455, 66) concurrency-limits\n",
      "wrong Shape of passed values is (294, 45), indices imply (294, 66) Shadow\n",
      "wrong Shape of passed values is (1170, 45), indices imply (1170, 66) LeetCode-Sol-Res\n",
      "wrong Shape of passed values is (521, 45), indices imply (521, 66) dialogflow-android-client\n",
      "wrong Shape of passed values is (355, 45), indices imply (355, 66) jhipster-registry\n",
      "wrong Shape of passed values is (723, 45), indices imply (723, 66) dexmaker\n",
      "wrong Shape of passed values is (107, 45), indices imply (107, 66) react-native-webview\n",
      "wrong Shape of passed values is (500, 45), indices imply (500, 66) TableView\n",
      "wrong Shape of passed values is (348, 45), indices imply (348, 66) contrib-drivers\n",
      "wrong Shape of passed values is (486, 45), indices imply (486, 66) EclipseCodeFormatter\n",
      "wrong No columns to parse from file pf4j\n",
      "wrong Shape of passed values is (651, 45), indices imply (651, 66) Android-ObservableScrollView\n",
      "wrong Shape of passed values is (3059, 45), indices imply (3059, 66) cw-omnibus\n",
      "wrong No columns to parse from file cocos2d\n",
      "wrong Shape of passed values is (928, 45), indices imply (928, 66) jpush-api-java-client\n",
      "wrong Shape of passed values is (8930, 45), indices imply (8930, 66) coding2017\n",
      "wrong Shape of passed values is (163, 45), indices imply (163, 66) Android-Debug-Database\n",
      "wrong Shape of passed values is (333, 45), indices imply (333, 66) java\n",
      "wrong Shape of passed values is (1003, 45), indices imply (1003, 66) jasmine-maven-plugin\n",
      "wrong Shape of passed values is (1039, 45), indices imply (1039, 66) android_sdk\n",
      "wrong Shape of passed values is (25, 45), indices imply (25, 66) yang\n",
      "wrong Shape of passed values is (1265, 45), indices imply (1265, 66) clickhouse-jdbc\n",
      "wrong Shape of passed values is (884, 45), indices imply (884, 66) hackerskeyboard\n",
      "wrong Shape of passed values is (860, 45), indices imply (860, 66) consul-client\n",
      "wrong 'file_name' jfairy\n",
      "wrong Shape of passed values is (7194, 45), indices imply (7194, 66) product-ei\n",
      "wrong Shape of passed values is (4096, 45), indices imply (4096, 66) restlet-framework-java\n",
      "wrong Shape of passed values is (3024, 45), indices imply (3024, 66) openmrs-contrib-android-client\n",
      "wrong Shape of passed values is (1918, 45), indices imply (1918, 66) SecurityShepherd\n",
      "wrong Shape of passed values is (211, 45), indices imply (211, 66) programming\n",
      "wrong Shape of passed values is (3997, 45), indices imply (3997, 66) OpenRTS\n",
      "wrong Shape of passed values is (1895, 45), indices imply (1895, 66) spectator\n",
      "wrong Shape of passed values is (3700, 45), indices imply (3700, 66) storio\n",
      "wrong Shape of passed values is (6808, 45), indices imply (6808, 66) logisim-evolution\n",
      "wrong 'DataFrame' object has no attribute 'Name' Medusa\n",
      "wrong Shape of passed values is (1621, 45), indices imply (1621, 66) spikes\n",
      "wrong 'DataFrame' object has no attribute 'Name' zotfile\n",
      "wrong Shape of passed values is (4015, 45), indices imply (4015, 66) parboiled\n",
      "wrong Shape of passed values is (3432, 45), indices imply (3432, 66) roboguice\n",
      "wrong Shape of passed values is (1213, 45), indices imply (1213, 66) netty-socketio\n",
      "wrong Shape of passed values is (1811, 45), indices imply (1811, 66) pi4j\n",
      "wrong Shape of passed values is (3287, 45), indices imply (3287, 66) find-sec-bugs\n",
      "wrong Shape of passed values is (1321, 45), indices imply (1321, 66) mongo-hadoop\n",
      "wrong Shape of passed values is (1221, 45), indices imply (1221, 66) javacpp\n",
      "wrong Shape of passed values is (4105, 45), indices imply (4105, 66) HoloEverywhere\n",
      "wrong Shape of passed values is (1064, 45), indices imply (1064, 66) lucida\n",
      "wrong Shape of passed values is (4407, 45), indices imply (4407, 66) RxNetty\n",
      "wrong Shape of passed values is (3018, 45), indices imply (3018, 66) xmemcached\n",
      "wrong Shape of passed values is (1622, 45), indices imply (1622, 66) guacamole-client\n",
      "wrong Shape of passed values is (3961, 45), indices imply (3961, 66) uhabits\n",
      "wrong Shape of passed values is (2731, 45), indices imply (2731, 66) spring-kafka\n",
      "wrong Shape of passed values is (3939, 45), indices imply (3939, 66) HiBench\n",
      "wrong Shape of passed values is (1491, 45), indices imply (1491, 66) XposedInstaller\n",
      "wrong Shape of passed values is (2531, 45), indices imply (2531, 66) rtmp-rtsp-stream-client-java\n",
      "wrong Shape of passed values is (2375, 45), indices imply (2375, 66) opsu\n",
      "wrong Shape of passed values is (5962, 45), indices imply (5962, 66) qbit\n",
      "wrong Shape of passed values is (272, 45), indices imply (272, 66) phoenicis\n"
     ]
    }
   ],
   "source": [
    "proj_df = pd.read_csv('projects.csv')\n",
    "projects = proj_df.repo_name.tolist()\n",
    "print(len(projects))\n",
    "for project in projects[150:]:\n",
    "    try:\n",
    "        load_data_commit_level(project,'process')\n",
    "    except Exception as e:\n",
    "        print('wrong',e ,project)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_release(project,metric):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    pf = []\n",
    "    f1 = []\n",
    "    g_score = []\n",
    "    auc = []\n",
    "    pci_20 = []\n",
    "    ifa = []\n",
    "    X_train,y_train,test_df = load_data_commit_level(project,metric)      \n",
    "    df_smote = pd.concat([X_train,y_train],axis = 1)\n",
    "    df_smote = apply_smote(df_smote)\n",
    "    y_train = df_smote.Bugs\n",
    "    X_train = df_smote.drop('Bugs',axis = 1)\n",
    "    clf =  RandomForestClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "    importance = clf.feature_importances_\n",
    "    unique_commits_list = np.array_split(test_df.commit_hash.unique(), 5)\n",
    "    for i in range(len(unique_commits_list)):\n",
    "        test_df_subset = test_df[test_df.commit_hash.isin(unique_commits_list[i])]\n",
    "        y_test = test_df_subset.Bugs\n",
    "        X_test = test_df_subset.drop(['Bugs','commit_hash'],axis = 1)\n",
    "        if metric == 'process':\n",
    "            loc = X_test['file_la'] + X_test['file_lt']\n",
    "        elif metric == 'product':\n",
    "            loc = X_test.CountLineCode\n",
    "        else:\n",
    "            loc = X_test['file_la'] + X_test['file_lt']                 \n",
    "        predicted = clf.predict(X_test)\n",
    "        abcd = metrices.measures(y_test,predicted,loc)\n",
    "        pf.append(abcd.get_pf())\n",
    "        recall.append(abcd.calculate_recall())\n",
    "        precision.append(abcd.calculate_precision())\n",
    "        f1.append(abcd.calculate_f1_score())\n",
    "        g_score.append(abcd.get_g_score())\n",
    "        pci_20.append(abcd.get_pci_20())\n",
    "        ifa.append(abcd.get_ifa())\n",
    "        try:\n",
    "            auc.append(roc_auc_score(y_test, predicted))\n",
    "        except:\n",
    "            auc.append(0)\n",
    "        print(classification_report(y_test, predicted))\n",
    "#         print(recall,precision,pf,f1,g_score,auc,pci_20)\n",
    "    return recall,precision,pf,f1,g_score,auc,pci_20,ifa,importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_k(project,metric):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    pf = []\n",
    "    f1 = []\n",
    "    g_score = []\n",
    "    auc = []\n",
    "    pci_20 = []\n",
    "    ifa = []\n",
    "    importance = []\n",
    "    X,y = load_both_data(project,metric)\n",
    "    for _ in range(5):\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "            y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "            if metric == 'process':\n",
    "                loc = X_test['file_la'] + X_test['file_lt']\n",
    "            elif metric == 'product':\n",
    "                loc = X_test.CountLineCode\n",
    "            else:\n",
    "                loc = X_test['file_la'] + X_test['file_lt']\n",
    "            df_smote = pd.concat([X_train,y_train],axis = 1)\n",
    "            df_smote = apply_smote(df_smote)\n",
    "            y_train = df_smote.Bugs\n",
    "            X_train = df_smote.drop('Bugs',axis = 1)\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X_train,y_train)\n",
    "            importance = clf.coef_ # change for RF\n",
    "            predicted = clf.predict(X_test)\n",
    "            abcd = metrices.measures(y_test,predicted,loc)\n",
    "            pf.append(abcd.get_pf())\n",
    "            recall.append(abcd.calculate_recall())\n",
    "            precision.append(abcd.calculate_precision())\n",
    "            f1.append(abcd.calculate_f1_score())\n",
    "            g_score.append(abcd.get_g_score())\n",
    "            pci_20.append(abcd.get_pci_20())\n",
    "            ifa.append(abcd.get_ifa())\n",
    "            try:\n",
    "                auc.append(roc_auc_score(y_test, predicted))\n",
    "            except:\n",
    "                auc.append(0)\n",
    "#             print(classification_report(y_test, predicted))\n",
    "    return recall,precision,pf,f1,g_score,auc,pci_20,ifa,importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self(project,metric):\n",
    "    X,y = load_both_data(project,metric)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=18)\n",
    "    if metric == 'process':\n",
    "        loc = X_test['la'] + X_test['lt']\n",
    "    elif metric == 'product':\n",
    "        loc = X_test.CountLineCode\n",
    "    else:\n",
    "        loc = X_test['la'] + X_test['lt']\n",
    "    df_smote = pd.concat([X_train,y_train],axis = 1)\n",
    "    df_smote = apply_smote(df_smote)\n",
    "    y_train = df_smote.Bugs\n",
    "    X_train = df_smote.drop('Bugs',axis = 1)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "    importance = clf.feature_importances_\n",
    "    print(len(importance))\n",
    "    predicted = clf.predict(X_test)\n",
    "    abcd = metrices.measures(y_test,predicted,loc)\n",
    "    pf = abcd.get_pf()\n",
    "    recall = abcd.calculate_recall()\n",
    "    precision = abcd.calculate_precision()\n",
    "    f1 = abcd.calculate_f1_score()\n",
    "    g_score = abcd.get_g_score()\n",
    "    pci_20 = abcd.get_pci_20()\n",
    "    ifa = abcd.get_ifa()\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, predicted)\n",
    "    except:\n",
    "        auc = 0\n",
    "    print(classification_report(y_test, predicted))\n",
    "    return recall,precision,pf,f1,g_score,auc,pci_20,ifa,importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_df = pd.read_csv('projects.csv')\n",
    "projects = proj_df.repo_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = {}\n",
    "recall_list = {}\n",
    "pf_list = {}\n",
    "f1_list = {}\n",
    "g_list = {}\n",
    "auc_list = {}\n",
    "pci_20_list = {}\n",
    "ifa_list = {}\n",
    "featue_importance = {}\n",
    "for project in projects[150:]:\n",
    "    try:\n",
    "        if project == '.DS_Store':\n",
    "            continue\n",
    "        print(\"+++++++++++++++++   \"  + project + \"  +++++++++++++++++\")\n",
    "        recall,precision,pf,f1,g_score,auc,pci_20,ifa,importance = run_self_release(project,'process')\n",
    "        recall_list[project] = recall\n",
    "        precision_list[project] = precision\n",
    "        pf_list[project] = pf\n",
    "        f1_list[project] = f1\n",
    "        g_list[project] = g_score\n",
    "        auc_list[project] = auc\n",
    "        pci_20_list[project] = pci_20\n",
    "        ifa_list[project] = ifa\n",
    "        featue_importance[project] = importance\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "final_result = {}\n",
    "final_result['precision'] = precision_list\n",
    "final_result['recall'] = recall_list\n",
    "final_result['pf'] = pf_list\n",
    "final_result['f1'] = f1_list\n",
    "final_result['g'] = g_list\n",
    "final_result['auc'] = auc_list\n",
    "final_result['pci_20'] = pci_20_list\n",
    "final_result['ifa'] = ifa_list\n",
    "final_result['featue_importance'] = featue_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/Performance/commit_guru_file_specific/process_700_rf_release.pkl', 'wb') as handle:\n",
    "    pickle.dump(final_result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release wise stats test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "dfs = ['process','product','process+product']\n",
    "orders = [\"P\", \"C\", \"P+C\"]\n",
    "all_results = pd.DataFrame()\n",
    "for k in range(len(dfs)):\n",
    "    df = dfs[k]\n",
    "    order = orders[k]\n",
    "    result_df = pd.DataFrame()\n",
    "    final_result = pd.read_pickle('results/Performance/commit_guru_file_specific/' + df +'_700_rf_release_devanvu_set3.pkl')\n",
    "    for metric in final_result.keys():\n",
    "        final_df = pd.DataFrame()\n",
    "        release = [[],[],[]]\n",
    "        for projects in final_result[metric].keys():\n",
    "            if len(final_result[metric][projects]) < 3:\n",
    "                continue\n",
    "            count += 1\n",
    "            i = 0\n",
    "            for value in final_result[metric][projects]:\n",
    "                if metric == 'ifa':\n",
    "                    value = value/100\n",
    "                release[i].append(value)\n",
    "                i += 1\n",
    "                if i == 3:\n",
    "                    break\n",
    "        for j in range(3):\n",
    "            score_df = pd.DataFrame(release[j], columns = ['scores'])\n",
    "            score_df['release'] = [j+1]*score_df.shape[0]\n",
    "            final_df = pd.concat([final_df,score_df], axis = 0)\n",
    "        final_df['metrics'] = [metric]*final_df.shape[0]\n",
    "        result_df = pd.concat([result_df,final_df], axis = 0)\n",
    "    result_df['Metric Type'] = [order]*result_df.shape[0]\n",
    "    all_results = pd.concat([all_results,result_df])\n",
    "all_results = all_results[all_results['metrics'] != 'featue_importance']\n",
    "all_results = all_results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in all_results.metrics.unique():\n",
    "    sub_df = all_results[all_results['metrics'] == metric]\n",
    "    with open('results/Stats/Release_stability/' + metric + '.txt', 'w') as f:\n",
    "        for mType in sub_df['Metric Type'].unique():\n",
    "            sub_sub_df = sub_df[sub_df['Metric Type'] == mType]\n",
    "            for rel in sub_sub_df.release.unique():\n",
    "                f.write(mType + \"_\" + str(rel) + \"\\n\")\n",
    "                sub_sub_sub_df = sub_sub_df[sub_sub_df['release'] == rel]\n",
    "                for score in sub_sub_sub_df.scores.values:\n",
    "                    f.write(\"%s \" % score)\n",
    "                f.write(\"\\n\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation Stats Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suvodeepmajumder/anaconda3/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1113: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/suvodeepmajumder/anaconda3/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1113: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/suvodeepmajumder/anaconda3/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1113: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "metrics = ['process','product','process+product']\n",
    "exp_types = ['JIT','release']\n",
    "all_results = pd.DataFrame()\n",
    "for exp_type in exp_types:\n",
    "    final_df = pd.DataFrame()\n",
    "    for metric in metrics:\n",
    "        project_corr = pd.read_pickle('results/Performance/Correlations/' + metric + '_corr_' + exp_type + '.pkl')\n",
    "        flat_list = [np.nanmedian(sublist) for sublist in list(project_corr.values())]\n",
    "        project_corr_df = pd.DataFrame(flat_list, columns = ['scores'])\n",
    "        project_corr_df['metrics'] = [metric]*project_corr_df.shape[0]\n",
    "        final_df = pd.concat([final_df,project_corr_df], axis = 0)\n",
    "    final_df['type'] = [exp_type]*final_df.shape[0]\n",
    "    all_results = pd.concat([all_results,final_df], axis = 0)\n",
    "all_results = all_results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/Stats/Correlations/file_corr.txt', 'w') as f:\n",
    "    for metric in all_results.metrics.unique():\n",
    "        sub_df = all_results[all_results['metrics'] == metric]\n",
    "        for _type in sub_df.type.unique():\n",
    "            sub_sub_df = sub_df[sub_df['type'] == _type]\n",
    "            f.write(metric + \"_\" + _type + \"\\n\")\n",
    "            for score in sub_sub_df.scores.values:\n",
    "                f.write(\"%s \" % score)\n",
    "            f.write(\"\\n\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
